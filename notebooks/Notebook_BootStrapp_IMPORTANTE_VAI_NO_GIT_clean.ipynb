{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "5a12a515945d4e67909218a0ac9fdeba": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0daa37c39d1c47408e632baaf939ec26",
        "IPY_MODEL_cfa3bd7334df4215b628f670108ec918",
        "IPY_MODEL_01cd6694d44842179b6168f734d9d482"
       ],
       "layout": "IPY_MODEL_fa01cbe07b5b4c8e9aa9477bd4e6a276"
      }
     },
     "0daa37c39d1c47408e632baaf939ec26": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72757d9cad214091a667972e108c743a",
       "placeholder": "​",
       "style": "IPY_MODEL_d8222ff559aa42e181568c1dc8a7da13",
       "value": "  0%"
      }
     },
     "cfa3bd7334df4215b628f670108ec918": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8d6b32c9344745d9ba55ae9e8d832492",
       "max": 10000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b56de5996fec4e4fa4c7e89f1bf97b8c",
       "value": 17
      }
     },
     "01cd6694d44842179b6168f734d9d482": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f722ecf9f9754c8f8adcbf837aba1c11",
       "placeholder": "​",
       "style": "IPY_MODEL_6e9825ec3b804cdc8fa29bafbcb17f7f",
       "value": " 17/10000 [00:07&lt;1:19:58,  2.08it/s]"
      }
     },
     "fa01cbe07b5b4c8e9aa9477bd4e6a276": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72757d9cad214091a667972e108c743a": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8222ff559aa42e181568c1dc8a7da13": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8d6b32c9344745d9ba55ae9e8d832492": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b56de5996fec4e4fa4c7e89f1bf97b8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f722ecf9f9754c8f8adcbf837aba1c11": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e9825ec3b804cdc8fa29bafbcb17f7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzV3ooYIsyRX",
    "outputId": "d461018d-d34a-4fe1-b4d3-8a10ce79b5c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9kMr7xPpBzC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('DATA')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Leaf Level Metrics"
   ],
   "metadata": {
    "id": "QZ_Bj0JviIcQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Micro-Metrics**"
   ],
   "metadata": {
    "id": "hZQbpYwPngkh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Etapa 2: Definir Nomes das Colunas ---\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "\n",
    "def parse_and_clean_codes(cell_content):\n",
    "\n",
    "    # 1. Se o dado já for uma lista (ou array numpy), processa cada item dela\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        # Filtra valores nulos (NaN) que podem estar dentro da lista\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "\n",
    "    # 2. Se não for uma lista, usa a lógica anterior para tratar como texto (string)\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "# --- Etapa 4: Loop de Cálculo (sem alterações aqui) ---\n",
    "results = {}\n",
    "print(\"Iniciando o cálculo das métricas.\")\n",
    "\n",
    "for model in model_cols:\n",
    "    if model not in df.columns:\n",
    "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
    "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "        tp = len(true_labels.intersection(predicted_labels))\n",
    "        fp = len(predicted_labels.difference(true_labels))\n",
    "        fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    results[model] = {\n",
    "        'Micro-Precision': micro_precision,\n",
    "        'Micro-Recall': micro_recall,\n",
    "        'Micro-F1': micro_f1\n",
    "    }\n",
    "    print(f\"Métricas para '{model}' calculadas.\")\n",
    "\n",
    "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='Micro-F1', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (Correspondência Exata)\")\n",
    "print(\"=\"*50)\n",
    "print(results_df)"
   ],
   "metadata": {
    "id": "5k89Wn7JiMEQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Macro-Metrics**"
   ],
   "metadata": {
    "id": "lj4qvBNEn3pr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Etapa 2: Definir Nomes das Colunas ---\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "\n",
    "# --- Etapa 3: Função de Limpeza (a mesma de antes) ---\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "\n",
    "# --- Etapa 4: Loop de Cálculo (LÓGICA MACRO) ---\n",
    "results_macro = {}\n",
    "print(\"Iniciando o cálculo das métricas Macro-Averaged...\")\n",
    "\n",
    "for model in model_cols:\n",
    "    if model not in df.columns:\n",
    "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    # Listas para armazenar as métricas de CADA linha (paciente)\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1 = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
    "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "        # Calcula TP, FP, FN para esta linha específica\n",
    "        tp = len(true_labels.intersection(predicted_labels))\n",
    "        fp = len(predicted_labels.difference(true_labels))\n",
    "        fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "        # Calcula as métricas para ESTA LINHA, com cuidado para não dividir por zero\n",
    "        precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row) if (precision_row + recall_row) > 0 else 0\n",
    "\n",
    "        # Adiciona as métricas da linha às listas\n",
    "        list_precision.append(precision_row)\n",
    "        list_recall.append(recall_row)\n",
    "        list_f1.append(f1_row)\n",
    "\n",
    "    # Após percorrer todas as linhas, calcula a média das métricas\n",
    "    macro_precision = np.mean(list_precision)\n",
    "    macro_recall = np.mean(list_recall)\n",
    "    macro_f1 = np.mean(list_f1)\n",
    "\n",
    "    results_macro[model] = {\n",
    "        'Macro-Precision': macro_precision,\n",
    "        'Macro-Recall': macro_recall,\n",
    "        'Macro-F1': macro_f1\n",
    "    }\n",
    "    print(f\"Métricas Macro para '{model}' calculadas.\")\n",
    "\n",
    "\n",
    "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
    "results_macro_df = pd.DataFrame(results_macro).T.sort_values(by='Macro-F1', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (Métricas Macro-Averaged)\")\n",
    "print(\"=\"*50)\n",
    "print(results_macro_df)\n",
    "\n",
    "try:\n",
    "    results_macro_df.to_csv(\"resultados_finais_macro.csv\")\n",
    "    print(\"\\n[✓] Resultados salvos com sucesso no arquivo 'resultados_finais_macro.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[X] Ocorreu um erro ao salvar o arquivo: {e}\")\n"
   ],
   "metadata": {
    "id": "Sh1aYJBrmVTa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Three-Character Metrics"
   ],
   "metadata": {
    "id": "xpvz6uNJn9ZA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Cleansing**"
   ],
   "metadata": {
    "id": "EwZgoWpyozEq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Etapa 3: Funções de Limpeza e Transformação ---\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    \"\"\"Lê uma célula (texto ou lista) e retorna um conjunto de códigos limpos.\"\"\"\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "def truncate_codes_in_cell(cell_content):\n",
    "    \"\"\"Usa a função anterior para ler os códigos e retorna uma NOVA LISTA com os códigos truncados.\"\"\"\n",
    "    full_codes_set = parse_and_clean_codes(cell_content)\n",
    "    # Pega cada código do conjunto, fatia os 3 primeiros caracteres e retorna como uma lista\n",
    "    truncated_codes_list = [code[:3] for code in full_codes_set]\n",
    "    return truncated_codes_list\n",
    "\n",
    "\n",
    "# --- Etapa 4: Aplicar a Transformação em df2 ---\n",
    "\n",
    "# Lista de todas as colunas que contêm códigos CID\n",
    "columns_to_transform = [\n",
    "    '21. CID de Alta', 'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "print(\"Iniciando a transformação para remover a especificidade (3 caracteres)...\")\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    if col in df2.columns:\n",
    "        # Aplica a função de truncar em cada célula da coluna especificada\n",
    "        df2[col] = df2[col].apply(truncate_codes_in_cell)\n",
    "        print(f\"Coluna '{col}' transformada.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DATA FRAME ORIGINAL (df)\")\n",
    "print(\"=\"*50)\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  NOVO DATAFRAME (df2) - SEM ESPECIFICIDADE (3 CARACTERES)\")\n",
    "print(\"=\"*50)\n",
    "print(df2)"
   ],
   "metadata": {
    "id": "0KjLHbmeoJgA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Micro-Metrics**"
   ],
   "metadata": {
    "id": "YvD0N6Vqo-po"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Etapa 2: Definir Nomes das Colunas ---\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "\n",
    "def parse_and_clean_codes(cell_content):\n",
    "\n",
    "    # 1. Se o dado já for uma lista (ou array numpy), processa cada item dela\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        # Filtra valores nulos (NaN) que podem estar dentro da lista\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "\n",
    "    # 2. Se não for uma lista, usa a lógica anterior para tratar como texto (string)\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "# --- Etapa 4: Loop de Cálculo (sem alterações aqui) ---\n",
    "results = {}\n",
    "print(\"Iniciando o cálculo das métricas.\")\n",
    "\n",
    "for model in model_cols:\n",
    "    if model not in df2.columns:\n",
    "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
    "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "        tp = len(true_labels.intersection(predicted_labels))\n",
    "        fp = len(predicted_labels.difference(true_labels))\n",
    "        fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    results[model] = {\n",
    "        'Micro-Precision': micro_precision,\n",
    "        'Micro-Recall': micro_recall,\n",
    "        'Micro-F1': micro_f1\n",
    "    }\n",
    "    print(f\"Métricas para '{model}' calculadas.\")\n",
    "\n",
    "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
    "results_df2 = pd.DataFrame(results).T.sort_values(by='Micro-F1', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (Correspondência Exata)\")\n",
    "print(\"=\"*50)\n",
    "print(results_df2)"
   ],
   "metadata": {
    "id": "sb3Gu4O-kLRr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Macro-Metrics**"
   ],
   "metadata": {
    "id": "MHKDqdGZrHT6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Etapa 2: Definir Nomes das Colunas ---\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "\n",
    "# --- Etapa 3: Função de Limpeza (a mesma de antes) ---\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "\n",
    "# --- Etapa 4: Loop de Cálculo (LÓGICA MACRO) ---\n",
    "results_macro = {}\n",
    "print(\"Iniciando o cálculo das métricas Macro-Averaged...\")\n",
    "\n",
    "for model in model_cols:\n",
    "    if model not in df2.columns:\n",
    "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    # Listas para armazenar as métricas de CADA linha (paciente)\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1 = []\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
    "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "        # Calcula TP, FP, FN para esta linha específica\n",
    "        tp = len(true_labels.intersection(predicted_labels))\n",
    "        fp = len(predicted_labels.difference(true_labels))\n",
    "        fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "        # Calcula as métricas para ESTA LINHA, com cuidado para não dividir por zero\n",
    "        precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row) if (precision_row + recall_row) > 0 else 0\n",
    "\n",
    "        # Adiciona as métricas da linha às listas\n",
    "        list_precision.append(precision_row)\n",
    "        list_recall.append(recall_row)\n",
    "        list_f1.append(f1_row)\n",
    "\n",
    "    # Após percorrer todas as linhas, calcula a média das métricas\n",
    "    macro_precision = np.mean(list_precision)\n",
    "    macro_recall = np.mean(list_recall)\n",
    "    macro_f1 = np.mean(list_f1)\n",
    "\n",
    "    results_macro[model] = {\n",
    "        'Macro-Precision': macro_precision,\n",
    "        'Macro-Recall': macro_recall,\n",
    "        'Macro-F1': macro_f1\n",
    "    }\n",
    "    print(f\"Métricas Macro para '{model}' calculadas.\")\n",
    "\n",
    "\n",
    "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
    "results_macro_df2 = pd.DataFrame(results_macro).T.sort_values(by='Macro-F1', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (Métricas Macro-Averaged)\")\n",
    "print(\"=\"*50)\n",
    "print(results_macro_df2)\n"
   ],
   "metadata": {
    "id": "JQ0pUmD5rLqZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bootstraping"
   ],
   "metadata": {
    "id": "tijfGi9WPMRi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/tabela_auditoria_llms_11_06.xlsx', engine='openpyxl')\n",
    "\n",
    "# --- Extrair apenas os CIDs da coluna \"API Deep-Seek - Com Prompt\" ---\n",
    "\n",
    "import re\n",
    "\n",
    "col = \"API Deep-Seek - Sem Prompt\"\n",
    "cid_re = re.compile(r\"[A-Z][0-9]{2}(?:\\.[0-9A-Z]{1,3})?\")\n",
    "\n",
    "def extrair_cids(x):\n",
    "    # Une tudo em uma string (seja lista, dict serializado ou texto solto)\n",
    "    if isinstance(x, list):\n",
    "        txt = \" \".join(map(str, x))\n",
    "    else:\n",
    "        txt = str(x)\n",
    "    # Coleta todos os matches válidos\n",
    "    cids = cid_re.findall(txt)\n",
    "    # Remove duplicatas preservando a ordem\n",
    "    return list(dict.fromkeys(cids))\n",
    "\n",
    "df[col] = df[col].apply(extrair_cids)\n",
    "\n",
    "df[\"21. CID de Alta\"] = df[\"21. CID de Alta\"].astype(str).apply(\n",
    "    lambda x: [cid.strip() for cid in x.split() if cid.strip()]\n",
    ")\n",
    "colunas_cid = [ \"API Maritalk\",\t\"API Deep-Seek - Sem Prompt\",\t\"API modelo Tunado\",\t\"API GPT-Mini\",\t\"API GPT‑4o\",\t\"API Gemini\"]\n",
    "\n",
    "import ast\n",
    "\n",
    "for col in colunas_cid:\n",
    "    df[col] = df[col].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "    )\n",
    "\n",
    "import re\n",
    "\n",
    "padrao_cid = re.compile(r\"^[A-Z][0-9]{2}(\\.[0-9A-Z]{1,3})?$\")\n",
    "\n",
    "def filtrar_cids(lista):\n",
    "    if not isinstance(lista, list):\n",
    "        return []\n",
    "    return [item for item in lista if isinstance(item, str) and padrao_cid.match(item)]\n",
    "\n",
    "for col in colunas_cid:\n",
    "    df[col] = df[col].apply(filtrar_cids)\n",
    "\n",
    "# Incluindo a coluna \"21. CID de Alta\"\n",
    "todas_colunas = colunas_cid + [\"21. CID de Alta\"]\n",
    "\n",
    "# Renomeando a coluna específica\n",
    "df.rename(\n",
    "    columns={'API Deep-Seek - Sem Prompt': 'API Deep-Seek'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "df"
   ],
   "metadata": {
    "id": "_kj4NX8GQDlF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Leaf Level Metrics**"
   ],
   "metadata": {
    "id": "1PaViPcoJXN0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# Leaf-level: cálculo de métricas, bootstrap, ICs,\n",
    "# plots com nomes padronizados e salvamento no Drive\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURAÇÕES INICIAIS\n",
    "# -----------------------------\n",
    "# >>>> Garanta que 'df' já esteja carregado com as colunas abaixo <<<<\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk',\n",
    "    'API Deep-Seek',\n",
    "    'API modelo Tunado',\n",
    "    'API GPT-Mini',\n",
    "    'API GPT‑4o',\n",
    "    'API Gemini'\n",
    "]\n",
    "metrics_to_calculate = [\n",
    "    'Micro-Precision', 'Micro-Recall', 'Micro-F1',\n",
    "    'Macro-Precision', 'Macro-Recall', 'Macro-F1'\n",
    "]\n",
    "\n",
    "# Diretório de saída (Leaf level)\n",
    "output_dir = \"/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Bootstrap/Leaf level\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# FUNÇÕES AUXILIARES\n",
    "# -----------------------------\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = (cell_content.strip()\n",
    "                               .replace('[', '')\n",
    "                               .replace(']', '')\n",
    "                               .replace(\"'\", \"\")\n",
    "                               .replace('\"', ''))\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "def calculate_all_metrics_micro_and_macro(dataframe, models, gold_standard):\n",
    "    final_results = {}\n",
    "    for model in models:\n",
    "        total_tp, total_fp, total_fn = 0, 0, 0\n",
    "        list_precision, list_recall, list_f1 = [], [], []\n",
    "\n",
    "        for _, row in dataframe.iterrows():\n",
    "            true_labels = parse_and_clean_codes(row[gold_standard])\n",
    "            predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "            tp = len(true_labels.intersection(predicted_labels))\n",
    "            fp = len(predicted_labels.difference(true_labels))\n",
    "            fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "            precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1_row = (2 * precision_row * recall_row / (precision_row + recall_row)\n",
    "                      if (precision_row + recall_row) > 0 else 0.0)\n",
    "            list_precision.append(precision_row)\n",
    "            list_recall.append(recall_row)\n",
    "            list_f1.append(f1_row)\n",
    "\n",
    "        results = {}\n",
    "        results['Micro-Precision'] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "        results['Micro-Recall']    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "        mp, mr = results['Micro-Precision'], results['Micro-Recall']\n",
    "        results['Micro-F1']        = 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0\n",
    "        results['Macro-Precision'] = float(np.mean(list_precision)) if list_precision else 0.0\n",
    "        results['Macro-Recall']    = float(np.mean(list_recall)) if list_recall else 0.0\n",
    "        results['Macro-F1']        = float(np.mean(list_f1)) if list_f1 else 0.0\n",
    "\n",
    "        final_results[model] = results\n",
    "\n",
    "    return pd.DataFrame(final_results).T\n",
    "\n",
    "# -----------------------------\n",
    "# ESTIMATIVAS PONTUAIS\n",
    "# -----------------------------\n",
    "print(\"Calculando as estimativas pontuais (6 métricas no dataset completo) [Leaf level]...\")\n",
    "point_estimates_df = calculate_all_metrics_micro_and_macro(df, model_cols, gold_standard_col)\n",
    "\n",
    "# -----------------------------\n",
    "# BOOTSTRAP\n",
    "# -----------------------------\n",
    "n_bootstraps = 10000\n",
    "print(f\"\\nIniciando o bootstrapping com {n_bootstraps} iterações para todas as 6 métricas [Leaf level]...\")\n",
    "bootstrap_scores = {model: {metric: [] for metric in metrics_to_calculate} for model in model_cols}\n",
    "\n",
    "for i in tqdm(range(n_bootstraps)):\n",
    "    df_resampled = df.sample(n=len(df), replace=True)\n",
    "    resampled_results = calculate_all_metrics_micro_and_macro(df_resampled, model_cols, gold_standard_col)\n",
    "    for model in model_cols:\n",
    "        for metric in metrics_to_calculate:\n",
    "            bootstrap_scores[model][metric].append(float(resampled_results.loc[model, metric]))\n",
    "\n",
    "print(\"Cálculo do bootstrapping concluído.\")\n",
    "\n",
    "# -----------------------------\n",
    "# CIs E TABELA FINAL\n",
    "# -----------------------------\n",
    "final_results_with_ci = {}\n",
    "alpha = (1.0 - 0.95) / 2.0\n",
    "\n",
    "for model in model_cols:\n",
    "    model_results = {}\n",
    "    for metric in metrics_to_calculate:\n",
    "        scores = bootstrap_scores[model][metric]\n",
    "        lower_bound = float(np.percentile(scores, alpha * 100))\n",
    "        upper_bound = float(np.percentile(scores, (1 - alpha) * 100))\n",
    "        model_results[metric] = float(point_estimates_df.loc[model, metric])\n",
    "        model_results[f'{metric} IC 95%'] = f\"[{lower_bound:.4f}, {upper_bound:.4f}]\"\n",
    "    final_results_with_ci[model] = model_results\n",
    "\n",
    "final_df = pd.DataFrame(final_results_with_ci).T\n",
    "final_df = final_df.sort_values(by='Micro-F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (MICRO E MACRO) COM INTERVALO DE CONFIANÇA (IC) — Leaf level\")\n",
    "print(\"=\"*80)\n",
    "print(final_df)\n",
    "\n",
    "# Salvar tabela com ICs (valores separados) no Drive\n",
    "final_csv_path = os.path.join(output_dir, \"final_results_with_CI.csv\")\n",
    "final_df.to_csv(final_csv_path, index=True)\n",
    "print(f\"\\nTabela final com IC salva: {final_csv_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MAPEAMENTO DE NOMES PARA PLOTS\n",
    "# -----------------------------\n",
    "name_map = {\n",
    "    'API Maritalk': 'Sabiá-3.1',\n",
    "    'API Deep-Seek': 'DeepSeek-V3',\n",
    "    'API modelo Tunado': 'Fine-Tuned GPT-4o-Mini',\n",
    "    'API GPT-Mini': 'GPT-4o-Mini',\n",
    "    'API GPT-4o': 'GPT-4o',\n",
    "    'API GPT-4o': 'GPT-4o',  # caso a coluna tenha U+202F\n",
    "    'API Gemini': 'Gemini-1.5 Flash',\n",
    "}\n",
    "metric_label_map = {\n",
    "    'Micro-Precision': 'Micro Precision',\n",
    "    'Micro-Recall': 'Micro Recall',\n",
    "    'Micro-F1': 'Micro F1-Score',\n",
    "    'Macro-Precision': 'Macro Precision',\n",
    "    'Macro-Recall': 'Macro Recall',\n",
    "    'Macro-F1': 'Macro F1-Score',\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# PLOTS + SALVAMENTO\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating distribution plots for each metric (Leaf level) and saving to Drive...\")\n",
    "for metric in metrics_to_calculate:\n",
    "    fig, axes = plt.subplots(1, len(model_cols), figsize=(len(model_cols) * 6, 5), sharey=True)\n",
    "\n",
    "    suptitle_txt = metric_label_map.get(metric, metric)\n",
    "    fig.suptitle(f'Bootstrap Distribution for {suptitle_txt}', fontsize=16)\n",
    "\n",
    "    # índice ordenado por Micro-F1 no final_df\n",
    "    for i, model in enumerate(final_df.index):\n",
    "        ax = axes[i]\n",
    "        scores = bootstrap_scores[model][metric]\n",
    "        sns.histplot(scores, kde=True, ax=ax, bins=30, stat=\"density\")\n",
    "\n",
    "        point_estimate = float(point_estimates_df.loc[model, metric])\n",
    "        ax.axvline(point_estimate, color='black', linestyle='-', linewidth=2,\n",
    "                   label=f'Point Estimate ({point_estimate:.4f})')\n",
    "\n",
    "        mean_bootstrap = float(np.mean(scores))\n",
    "        ax.axvline(mean_bootstrap, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Bootstrap Mean ({mean_bootstrap:.4f})')\n",
    "\n",
    "        lower_bound = float(np.percentile(scores, 2.5))\n",
    "        upper_bound = float(np.percentile(scores, 97.5))\n",
    "        ax.axvline(lower_bound, color='g', linestyle=':', linewidth=2, label='95% CI')\n",
    "        ax.axvline(upper_bound, color='g', linestyle=':', linewidth=2)\n",
    "\n",
    "        ax.set_title(name_map.get(model, model), fontsize=12)\n",
    "        ax.set_xlabel(metric_label_map.get(metric, metric))\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    safe_metric = metric.replace(' ', '_').replace('-', '')\n",
    "    fig_path = os.path.join(output_dir, f'distribution_bootstrap_{safe_metric}.png')\n",
    "    plt.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Chart saved: {fig_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# PE vs BOOTSTRAP MEAN + SALVAR\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for model in model_cols:\n",
    "    # garantir chave correta se houver variação em 'API GPT-4o'\n",
    "    model_key = model\n",
    "    if model_key not in point_estimates_df.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df.index:\n",
    "        model_key = 'API GPT-4o'\n",
    "    if model_key not in point_estimates_df.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df.index:\n",
    "        model_key = 'API GPT-4o'\n",
    "\n",
    "    for metric in metrics_to_calculate:\n",
    "        pe = float(point_estimates_df.loc[model_key, metric])\n",
    "        bm = float(np.mean(bootstrap_scores[model_key][metric]))\n",
    "        diff = bm - pe\n",
    "        rows.append({\n",
    "            'Model': name_map.get(model_key, model_key),\n",
    "            'Metric': metric_label_map.get(metric, metric),\n",
    "            'Point Estimate': f\"{pe:.4f}\",\n",
    "            'Bootstrap Mean': f\"{bm:.4f}\",\n",
    "            'Diff (BM-PE)': f\"{diff:.6f}\",\n",
    "        })\n",
    "\n",
    "pe_vs_bm_df = pd.DataFrame(rows)\n",
    "pe_vs_bm_df['abs_diff'] = pe_vs_bm_df['Diff (BM-PE)'].astype(float).abs()\n",
    "pe_vs_bm_df = pe_vs_bm_df.sort_values(by='abs_diff', ascending=False).drop(columns='abs_diff')\n",
    "\n",
    "pebm_csv_path = os.path.join(output_dir, \"point_estimate_vs_bootstrap_mean.csv\")\n",
    "pe_vs_bm_df.to_csv(pebm_csv_path, index=False)\n",
    "print(f\"\\nComparação PE vs BM salva: {pebm_csv_path}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440,
     "referenced_widgets": [
      "5a12a515945d4e67909218a0ac9fdeba",
      "0daa37c39d1c47408e632baaf939ec26",
      "cfa3bd7334df4215b628f670108ec918",
      "01cd6694d44842179b6168f734d9d482",
      "fa01cbe07b5b4c8e9aa9477bd4e6a276",
      "72757d9cad214091a667972e108c743a",
      "d8222ff559aa42e181568c1dc8a7da13",
      "8d6b32c9344745d9ba55ae9e8d832492",
      "b56de5996fec4e4fa4c7e89f1bf97b8c",
      "f722ecf9f9754c8f8adcbf837aba1c11",
      "6e9825ec3b804cdc8fa29bafbcb17f7f"
     ]
    },
    "id": "rQfZK3lGhtlq",
    "outputId": "5dd5748a-4094-4c09-b4a6-06d7ede04a32"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Calculando as estimativas pontuais (6 métricas no dataset completo) [Leaf level]...\n",
      "\n",
      "Iniciando o bootstrapping com 10000 iterações para todas as 6 métricas [Leaf level]...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a12a515945d4e67909218a0ac9fdeba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1909635906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bootstraps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mdf_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mresampled_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_all_metrics_micro_and_macro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_standard_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics_to_calculate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1909635906.py\u001b[0m in \u001b[0;36mcalculate_all_metrics_micro_and_macro\u001b[0;34m(dataframe, models, gold_standard)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mlist_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_and_clean_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgold_standard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_and_clean_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0musing_cow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musing_cow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Three-Character Category - Data Cleasing**"
   ],
   "metadata": {
    "id": "Y0fN4XReJkuq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Etapa 3: Funções de Limpeza e Transformação ---\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    \"\"\"Lê uma célula (texto ou lista) e retorna um conjunto de códigos limpos.\"\"\"\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "def truncate_codes_in_cell(cell_content):\n",
    "    \"\"\"Usa a função anterior para ler os códigos e retorna uma NOVA LISTA com os códigos truncados.\"\"\"\n",
    "    full_codes_set = parse_and_clean_codes(cell_content)\n",
    "    # Pega cada código do conjunto, fatia os 3 primeiros caracteres e retorna como uma lista\n",
    "    truncated_codes_list = [code[:3] for code in full_codes_set]\n",
    "    return truncated_codes_list\n",
    "\n",
    "\n",
    "# --- Etapa 4: Aplicar a Transformação em df2 ---\n",
    "\n",
    "# Lista de todas as colunas que contêm códigos CID\n",
    "columns_to_transform = [\n",
    "    '21. CID de Alta', 'API Maritalk', 'API Deep-Seek', 'API modelo Tunado',\n",
    "    'API GPT-Mini', 'API GPT‑4o', 'API Gemini'\n",
    "]\n",
    "\n",
    "print(\"Iniciando a transformação para remover a especificidade (3 caracteres)...\")\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    if col in df2.columns:\n",
    "        # Aplica a função de truncar em cada célula da coluna especificada\n",
    "        df2[col] = df2[col].apply(truncate_codes_in_cell)\n",
    "        print(f\"Coluna '{col}' transformada.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  DATA FRAME ORIGINAL (df)\")\n",
    "print(\"=\"*50)\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  NOVO DATAFRAME (df2) - SEM ESPECIFICIDADE (3 CARACTERES)\")\n",
    "print(\"=\"*50)\n",
    "print(df2)"
   ],
   "metadata": {
    "id": "GYcDBsW6WHF1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Three Category Metrics Bootstrap**"
   ],
   "metadata": {
    "id": "wTBfCf2qKJHM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURAÇÕES INICIAIS\n",
    "# -----------------------------\n",
    "# >>>> Garanta que 'df2' já esteja carregado com as colunas abaixo <<<<\n",
    "gold_standard_col = '21. CID de Alta'\n",
    "model_cols = [\n",
    "    'API Maritalk',\n",
    "    'API Deep-Seek',\n",
    "    'API modelo Tunado',\n",
    "    'API GPT-Mini',\n",
    "    'API GPT‑4o',\n",
    "    'API Gemini'\n",
    "]\n",
    "metrics_to_calculate = [\n",
    "    'Micro-Precision', 'Micro-Recall', 'Micro-F1',\n",
    "    'Macro-Precision', 'Macro-Recall', 'Macro-F1'\n",
    "]\n",
    "\n",
    "# Diretório de saída (Tree Category level)\n",
    "output_dir = \"/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Bootstrap/Three-character category\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# FUNÇÕES AUXILIARES\n",
    "# -----------------------------\n",
    "def parse_and_clean_codes(cell_content):\n",
    "    if isinstance(cell_content, (list, np.ndarray)):\n",
    "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
    "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
    "        return set()\n",
    "    cleaned_text = (cell_content.strip()\n",
    "                               .replace('[', '')\n",
    "                               .replace(']', '')\n",
    "                               .replace(\"'\", \"\")\n",
    "                               .replace('\"', ''))\n",
    "    if not cleaned_text:\n",
    "        return set()\n",
    "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
    "    return codes\n",
    "\n",
    "def calculate_all_metrics_micro_and_macro(dataframe, models, gold_standard):\n",
    "    final_results = {}\n",
    "    for model in models:\n",
    "        total_tp, total_fp, total_fn = 0, 0, 0\n",
    "        list_precision, list_recall, list_f1 = [], [], []\n",
    "\n",
    "        for _, row in dataframe.iterrows():\n",
    "            true_labels = parse_and_clean_codes(row[gold_standard])\n",
    "            predicted_labels = parse_and_clean_codes(row.get(model))\n",
    "\n",
    "            tp = len(true_labels.intersection(predicted_labels))\n",
    "            fp = len(predicted_labels.difference(true_labels))\n",
    "            fn = len(true_labels.difference(predicted_labels))\n",
    "\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "            precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1_row = (2 * precision_row * recall_row / (precision_row + recall_row)\n",
    "                      if (precision_row + recall_row) > 0 else 0.0)\n",
    "            list_precision.append(precision_row)\n",
    "            list_recall.append(recall_row)\n",
    "            list_f1.append(f1_row)\n",
    "\n",
    "        results = {}\n",
    "        results['Micro-Precision'] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "        results['Micro-Recall']    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "        mp, mr = results['Micro-Precision'], results['Micro-Recall']\n",
    "        results['Micro-F1']        = 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0\n",
    "        results['Macro-Precision'] = float(np.mean(list_precision)) if list_precision else 0.0\n",
    "        results['Macro-Recall']    = float(np.mean(list_recall)) if list_recall else 0.0\n",
    "        results['Macro-F1']        = float(np.mean(list_f1)) if list_f1 else 0.0\n",
    "\n",
    "        final_results[model] = results\n",
    "\n",
    "    return pd.DataFrame(final_results).T\n",
    "\n",
    "# -----------------------------\n",
    "# ESTIMATIVAS PONTUAIS\n",
    "# -----------------------------\n",
    "print(\"Calculando as estimativas pontuais (6 métricas no dataset completo) [Leaf level]...\")\n",
    "point_estimates_df2 = calculate_all_metrics_micro_and_macro(df2, model_cols, gold_standard_col)\n",
    "\n",
    "# -----------------------------\n",
    "# BOOTSTRAP\n",
    "# -----------------------------\n",
    "n_bootstraps = 10000\n",
    "print(f\"\\nIniciando o bootstrapping com {n_bootstraps} iterações para todas as 6 métricas [Leaf level]...\")\n",
    "bootstrap_scores = {model: {metric: [] for metric in metrics_to_calculate} for model in model_cols}\n",
    "\n",
    "for i in tqdm(range(n_bootstraps)):\n",
    "    df2_resampled = df2.sample(n=len(df2), replace=True)\n",
    "    resampled_results = calculate_all_metrics_micro_and_macro(df2_resampled, model_cols, gold_standard_col)\n",
    "    for model in model_cols:\n",
    "        for metric in metrics_to_calculate:\n",
    "            bootstrap_scores[model][metric].append(float(resampled_results.loc[model, metric]))\n",
    "\n",
    "print(\"Cálculo do bootstrapping concluído.\")\n",
    "\n",
    "# -----------------------------\n",
    "# CIs E TABELA FINAL\n",
    "# -----------------------------\n",
    "final_results_with_ci = {}\n",
    "alpha = (1.0 - 0.95) / 2.0\n",
    "\n",
    "for model in model_cols:\n",
    "    model_results = {}\n",
    "    for metric in metrics_to_calculate:\n",
    "        scores = bootstrap_scores[model][metric]\n",
    "        lower_bound = float(np.percentile(scores, alpha * 100))\n",
    "        upper_bound = float(np.percentile(scores, (1 - alpha) * 100))\n",
    "        model_results[metric] = float(point_estimates_df2.loc[model, metric])\n",
    "        model_results[f'{metric} IC 95%'] = f\"[{lower_bound:.4f}, {upper_bound:.4f}]\"\n",
    "    final_results_with_ci[model] = model_results\n",
    "\n",
    "final_df2 = pd.DataFrame(final_results_with_ci).T\n",
    "final_df2 = final_df2.sort_values(by='Micro-F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  DESEMPENHO FINAL DOS MODELOS (MICRO E MACRO) COM INTERVALO DE CONFIANÇA (IC) — Leaf level\")\n",
    "print(\"=\"*80)\n",
    "print(final_df2)\n",
    "\n",
    "# Salvar tabela com ICs (valores separados) no Drive\n",
    "final_csv_path = os.path.join(output_dir, \"final_results_with_CI.csv\")\n",
    "final_df2.to_csv(final_csv_path, index=True)\n",
    "print(f\"\\nTabela final com IC salva: {final_csv_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MAPEAMENTO DE NOMES PARA PLOTS\n",
    "# -----------------------------\n",
    "name_map = {\n",
    "    'API Maritalk': 'Sabiá-3.1',\n",
    "    'API Deep-Seek': 'DeepSeek-V3',\n",
    "    'API modelo Tunado': 'Fine-Tuned GPT-4o-Mini',\n",
    "    'API GPT-Mini': 'GPT-4o-Mini',\n",
    "    'API GPT-4o': 'GPT-4o',\n",
    "    'API GPT-4o': 'GPT-4o',  # caso a coluna tenha U+202F\n",
    "    'API Gemini': 'Gemini-1.5 Flash',\n",
    "}\n",
    "metric_label_map = {\n",
    "    'Micro-Precision': 'Micro Precision',\n",
    "    'Micro-Recall': 'Micro Recall',\n",
    "    'Micro-F1': 'Micro F1-Score',\n",
    "    'Macro-Precision': 'Macro Precision',\n",
    "    'Macro-Recall': 'Macro Recall',\n",
    "    'Macro-F1': 'Macro F1-Score',\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# PLOTS + SALVAMENTO\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating distribution plots for each metric (Leaf level) and saving to Drive...\")\n",
    "for metric in metrics_to_calculate:\n",
    "    fig, axes = plt.subplots(1, len(model_cols), figsize=(len(model_cols) * 6, 5), sharey=True)\n",
    "\n",
    "    suptitle_txt = metric_label_map.get(metric, metric)\n",
    "    fig.suptitle(f'Bootstrap Distribution for {suptitle_txt}', fontsize=16)\n",
    "\n",
    "    # índice ordenado por Micro-F1 no final_df2\n",
    "    for i, model in enumerate(final_df2.index):\n",
    "        ax = axes[i]\n",
    "        scores = bootstrap_scores[model][metric]\n",
    "        sns.histplot(scores, kde=True, ax=ax, bins=30, stat=\"density\")\n",
    "\n",
    "        point_estimate = float(point_estimates_df2.loc[model, metric])\n",
    "        ax.axvline(point_estimate, color='black', linestyle='-', linewidth=2,\n",
    "                   label=f'Point Estimate ({point_estimate:.4f})')\n",
    "\n",
    "        mean_bootstrap = float(np.mean(scores))\n",
    "        ax.axvline(mean_bootstrap, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Bootstrap Mean ({mean_bootstrap:.4f})')\n",
    "\n",
    "        lower_bound = float(np.percentile(scores, 2.5))\n",
    "        upper_bound = float(np.percentile(scores, 97.5))\n",
    "        ax.axvline(lower_bound, color='g', linestyle=':', linewidth=2, label='95% CI')\n",
    "        ax.axvline(upper_bound, color='g', linestyle=':', linewidth=2)\n",
    "\n",
    "        ax.set_title(name_map.get(model, model), fontsize=12)\n",
    "        ax.set_xlabel(metric_label_map.get(metric, metric))\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    safe_metric = metric.replace(' ', '_').replace('-', '')\n",
    "    fig_path = os.path.join(output_dir, f'distribution_bootstrap_{safe_metric}.png')\n",
    "    plt.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Chart saved: {fig_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# PE vs BOOTSTRAP MEAN + SALVAR\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for model in model_cols:\n",
    "    # garantir chave correta se houver variação em 'API GPT-4o'\n",
    "    model_key = model\n",
    "    if model_key not in point_estimates_df2.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df2.index:\n",
    "        model_key = 'API GPT-4o'\n",
    "    if model_key not in point_estimates_df2.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df2.index:\n",
    "        model_key = 'API GPT-4o'\n",
    "\n",
    "    for metric in metrics_to_calculate:\n",
    "        pe = float(point_estimates_df2.loc[model_key, metric])\n",
    "        bm = float(np.mean(bootstrap_scores[model_key][metric]))\n",
    "        diff = bm - pe\n",
    "        rows.append({\n",
    "            'Model': name_map.get(model_key, model_key),\n",
    "            'Metric': metric_label_map.get(metric, metric),\n",
    "            'Point Estimate': f\"{pe:.4f}\",\n",
    "            'Bootstrap Mean': f\"{bm:.4f}\",\n",
    "            'Diff (BM-PE)': f\"{diff:.6f}\",\n",
    "        })\n",
    "\n",
    "pe_vs_bm_df2 = pd.DataFrame(rows)\n",
    "pe_vs_bm_df2['abs_diff'] = pe_vs_bm_df2['Diff (BM-PE)'].astype(float).abs()\n",
    "pe_vs_bm_df2 = pe_vs_bm_df2.sort_values(by='abs_diff', ascending=False).drop(columns='abs_diff')\n",
    "\n",
    "pebm_csv_path = os.path.join(output_dir, \"point_estimate_vs_bootstrap_mean.csv\")\n",
    "pe_vs_bm_df2.to_csv(pebm_csv_path, index=False)\n",
    "print(f\"\\nComparação PE vs BM salva: {pebm_csv_path}\")\n"
   ],
   "metadata": {
    "id": "hoq8AJJlKVrT"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
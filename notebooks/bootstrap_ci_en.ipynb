{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrap Confidence Intervals for ICD-10 Coding Evaluation (English Translation)\n",
        "\n",
        "This notebook computes non-parametric bootstrap confidence intervals (95% CI)\n",
        "for precision, recall, and F1-score in automated ICD-10 coding experiments\n",
        "using **English-translated versions of the Portuguese obstetric clinical notes**.\n",
        "\n",
        "Bootstrap resampling is performed at the note level (n = 10,000 resamples),\n",
        "following standard practice for multi-label clinical NLP evaluation.\n",
        "Metrics are reported at both the three-character ICD-10 category level and\n",
        "the full (leaf-level) specificity.\n",
        "\n",
        "This analysis accompanies the paper:\n",
        "\"Large Language Models for Automated ICD-10 Coding of Obstetric Clinical Notes in Portuguese\".\n",
        "\n",
        "Clinical data are not distributed with this repository."
      ],
      "metadata": {
        "id": "SMfyPaRylWOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzV3ooYIsyRX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9kMr7xPpBzC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_pickle('/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Data/tabela_cids_inglesa_completa.pkl')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ_Bj0JviIcQ"
      },
      "source": [
        "#Leaf Level Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZQbpYwPngkh"
      },
      "source": [
        "**Micro-Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k89Wn7JiMEQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Etapa 2: Definir Nomes das Colunas ---\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "\n",
        "\n",
        "def parse_and_clean_codes(cell_content):\n",
        "\n",
        "    # 1. Se o dado já for uma lista (ou array numpy), processa cada item dela\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        # Filtra valores nulos (NaN) que podem estar dentro da lista\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "\n",
        "    # 2. Se não for uma lista, usa a lógica anterior para tratar como texto (string)\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "# --- Etapa 4: Loop de Cálculo (sem alterações aqui) ---\n",
        "results = {}\n",
        "print(\"Iniciando o cálculo das métricas.\")\n",
        "\n",
        "for model in model_cols:\n",
        "    if model not in df.columns:\n",
        "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
        "        continue\n",
        "\n",
        "    total_tp, total_fp, total_fn = 0, 0, 0\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
        "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "        tp = len(true_labels.intersection(predicted_labels))\n",
        "        fp = len(predicted_labels.difference(true_labels))\n",
        "        fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "\n",
        "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
        "\n",
        "    results[model] = {\n",
        "        'Micro-Precision': micro_precision,\n",
        "        'Micro-Recall': micro_recall,\n",
        "        'Micro-F1': micro_f1\n",
        "    }\n",
        "    print(f\"Métricas para '{model}' calculadas.\")\n",
        "\n",
        "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
        "results_df = pd.DataFrame(results).T.sort_values(by='Micro-F1', ascending=False)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (Correspondência Exata)\")\n",
        "print(\"=\"*50)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj4qvBNEn3pr"
      },
      "source": [
        "**Macro-Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh1aYJBrmVTa"
      },
      "outputs": [],
      "source": [
        "# --- Etapa 2: Definir Nomes das Colunas ---\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "\n",
        "\n",
        "# --- Etapa 3: Função de Limpeza (a mesma de antes) ---\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "\n",
        "# --- Etapa 4: Loop de Cálculo (LÓGICA MACRO) ---\n",
        "results_macro = {}\n",
        "print(\"Iniciando o cálculo das métricas Macro-Averaged...\")\n",
        "\n",
        "for model in model_cols:\n",
        "    if model not in df.columns:\n",
        "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
        "        continue\n",
        "\n",
        "    # Listas para armazenar as métricas de CADA linha (paciente)\n",
        "    list_precision = []\n",
        "    list_recall = []\n",
        "    list_f1 = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
        "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "        # Calcula TP, FP, FN para esta linha específica\n",
        "        tp = len(true_labels.intersection(predicted_labels))\n",
        "        fp = len(predicted_labels.difference(true_labels))\n",
        "        fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "        # Calcula as métricas para ESTA LINHA, com cuidado para não dividir por zero\n",
        "        precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row) if (precision_row + recall_row) > 0 else 0\n",
        "\n",
        "        # Adiciona as métricas da linha às listas\n",
        "        list_precision.append(precision_row)\n",
        "        list_recall.append(recall_row)\n",
        "        list_f1.append(f1_row)\n",
        "\n",
        "    # Após percorrer todas as linhas, calcula a média das métricas\n",
        "    macro_precision = np.mean(list_precision)\n",
        "    macro_recall = np.mean(list_recall)\n",
        "    macro_f1 = np.mean(list_f1)\n",
        "\n",
        "    results_macro[model] = {\n",
        "        'Macro-Precision': macro_precision,\n",
        "        'Macro-Recall': macro_recall,\n",
        "        'Macro-F1': macro_f1\n",
        "    }\n",
        "    print(f\"Métricas Macro para '{model}' calculadas.\")\n",
        "\n",
        "\n",
        "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
        "results_macro_df = pd.DataFrame(results_macro).T.sort_values(by='Macro-F1', ascending=False)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (Métricas Macro-Averaged)\")\n",
        "print(\"=\"*50)\n",
        "print(results_macro_df)\n",
        "\n",
        "try:\n",
        "    results_macro_df.to_csv(\"resultados_finais_macro.csv\")\n",
        "    print(\"\\n[✓] Resultados salvos com sucesso no arquivo 'resultados_finais_macro.csv'\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n[X] Ocorreu um erro ao salvar o arquivo: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpvz6uNJn9ZA"
      },
      "source": [
        "#Three-Character Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwZgoWpyozEq"
      },
      "source": [
        "**Data Cleansing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KjLHbmeoJgA"
      },
      "outputs": [],
      "source": [
        "# --- Etapa 3: Funções de Limpeza e Transformação ---\n",
        "\n",
        "df2 = df.copy()\n",
        "\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    \"\"\"Lê uma célula (texto ou lista) e retorna um conjunto de códigos limpos.\"\"\"\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "def truncate_codes_in_cell(cell_content):\n",
        "    \"\"\"Usa a função anterior para ler os códigos e retorna uma NOVA LISTA com os códigos truncados.\"\"\"\n",
        "    full_codes_set = parse_and_clean_codes(cell_content)\n",
        "    # Pega cada código do conjunto, fatia os 3 primeiros caracteres e retorna como uma lista\n",
        "    truncated_codes_list = [code[:3] for code in full_codes_set]\n",
        "    return truncated_codes_list\n",
        "\n",
        "\n",
        "# --- Etapa 4: Aplicar a Transformação em df2 ---\n",
        "\n",
        "# Lista de todas as colunas que contêm códigos CID\n",
        "columns_to_transform = [\n",
        "    '21. CID de Alta', 'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "\n",
        "print(\"Iniciando a transformação para remover a especificidade (3 caracteres)...\")\n",
        "\n",
        "for col in columns_to_transform:\n",
        "    if col in df2.columns:\n",
        "        # Aplica a função de truncar em cada célula da coluna especificada\n",
        "        df2[col] = df2[col].apply(truncate_codes_in_cell)\n",
        "        print(f\"Coluna '{col}' transformada.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DATA FRAME ORIGINAL (df)\")\n",
        "print(\"=\"*50)\n",
        "print(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  NOVO DATAFRAME (df2) - SEM ESPECIFICIDADE (3 CARACTERES)\")\n",
        "print(\"=\"*50)\n",
        "print(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvD0N6Vqo-po"
      },
      "source": [
        "**Micro-Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb3Gu4O-kLRr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Etapa 2: Definir Nomes das Colunas ---\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "\n",
        "\n",
        "def parse_and_clean_codes(cell_content):\n",
        "\n",
        "    # 1. Se o dado já for uma lista (ou array numpy), processa cada item dela\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        # Filtra valores nulos (NaN) que podem estar dentro da lista\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "\n",
        "    # 2. Se não for uma lista, usa a lógica anterior para tratar como texto (string)\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "# --- Etapa 4: Loop de Cálculo (sem alterações aqui) ---\n",
        "results = {}\n",
        "print(\"Iniciando o cálculo das métricas.\")\n",
        "\n",
        "for model in model_cols:\n",
        "    if model not in df2.columns:\n",
        "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
        "        continue\n",
        "\n",
        "    total_tp, total_fp, total_fn = 0, 0, 0\n",
        "\n",
        "    for index, row in df2.iterrows():\n",
        "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
        "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "        tp = len(true_labels.intersection(predicted_labels))\n",
        "        fp = len(predicted_labels.difference(true_labels))\n",
        "        fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "\n",
        "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
        "\n",
        "    results[model] = {\n",
        "        'Micro-Precision': micro_precision,\n",
        "        'Micro-Recall': micro_recall,\n",
        "        'Micro-F1': micro_f1\n",
        "    }\n",
        "    print(f\"Métricas para '{model}' calculadas.\")\n",
        "\n",
        "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
        "results_df2 = pd.DataFrame(results).T.sort_values(by='Micro-F1', ascending=False)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (Correspondência Exata)\")\n",
        "print(\"=\"*50)\n",
        "print(results_df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHKDqdGZrHT6"
      },
      "source": [
        "**Macro-Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ0pUmD5rLqZ"
      },
      "outputs": [],
      "source": [
        "# --- Etapa 2: Definir Nomes das Colunas ---\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "\n",
        "\n",
        "# --- Etapa 3: Função de Limpeza (a mesma de antes) ---\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "\n",
        "# --- Etapa 4: Loop de Cálculo (LÓGICA MACRO) ---\n",
        "results_macro = {}\n",
        "print(\"Iniciando o cálculo das métricas Macro-Averaged...\")\n",
        "\n",
        "for model in model_cols:\n",
        "    if model not in df2.columns:\n",
        "        print(f\"Aviso: A coluna '{model}' não foi encontrada. Pulando.\")\n",
        "        continue\n",
        "\n",
        "    # Listas para armazenar as métricas de CADA linha (paciente)\n",
        "    list_precision = []\n",
        "    list_recall = []\n",
        "    list_f1 = []\n",
        "\n",
        "    for index, row in df2.iterrows():\n",
        "        true_labels = parse_and_clean_codes(row[gold_standard_col])\n",
        "        predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "        # Calcula TP, FP, FN para esta linha específica\n",
        "        tp = len(true_labels.intersection(predicted_labels))\n",
        "        fp = len(predicted_labels.difference(true_labels))\n",
        "        fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "        # Calcula as métricas para ESTA LINHA, com cuidado para não dividir por zero\n",
        "        precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row) if (precision_row + recall_row) > 0 else 0\n",
        "\n",
        "        # Adiciona as métricas da linha às listas\n",
        "        list_precision.append(precision_row)\n",
        "        list_recall.append(recall_row)\n",
        "        list_f1.append(f1_row)\n",
        "\n",
        "    # Após percorrer todas as linhas, calcula a média das métricas\n",
        "    macro_precision = np.mean(list_precision)\n",
        "    macro_recall = np.mean(list_recall)\n",
        "    macro_f1 = np.mean(list_f1)\n",
        "\n",
        "    results_macro[model] = {\n",
        "        'Macro-Precision': macro_precision,\n",
        "        'Macro-Recall': macro_recall,\n",
        "        'Macro-F1': macro_f1\n",
        "    }\n",
        "    print(f\"Métricas Macro para '{model}' calculadas.\")\n",
        "\n",
        "\n",
        "# --- Etapa 5: Exibir e Salvar os Resultados Finais ---\n",
        "results_macro_df2 = pd.DataFrame(results_macro).T.sort_values(by='Macro-F1', ascending=False)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (Métricas Macro-Averaged)\")\n",
        "print(\"=\"*50)\n",
        "print(results_macro_df2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tijfGi9WPMRi"
      },
      "source": [
        "# Bootstraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PaViPcoJXN0"
      },
      "source": [
        "**Leaf Level Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rQfZK3lGhtlq"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Leaf-level: cálculo de métricas, bootstrap, ICs,\n",
        "# plots com nomes padronizados e salvamento no Drive\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURAÇÕES INICIAIS\n",
        "# -----------------------------\n",
        "# >>>> Garanta que 'df' já esteja carregado com as colunas abaixo <<<<\n",
        "\n",
        "df = pd.read_pickle('/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Data/tabela_cids_inglesa_completa.pkl')\n",
        "\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "metrics_to_calculate = [\n",
        "    'Micro-Precision', 'Micro-Recall', 'Micro-F1',\n",
        "    'Macro-Precision', 'Macro-Recall', 'Macro-F1'\n",
        "]\n",
        "\n",
        "# Diretório de saída (Leaf level)\n",
        "output_dir = \"/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Bootstrap/Leaf level\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# FUNÇÕES AUXILIARES\n",
        "# -----------------------------\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = (cell_content.strip()\n",
        "                               .replace('[', '')\n",
        "                               .replace(']', '')\n",
        "                               .replace(\"'\", \"\")\n",
        "                               .replace('\"', ''))\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "def calculate_all_metrics_micro_and_macro(dataframe, models, gold_standard):\n",
        "    final_results = {}\n",
        "    for model in models:\n",
        "        total_tp, total_fp, total_fn = 0, 0, 0\n",
        "        list_precision, list_recall, list_f1 = [], [], []\n",
        "\n",
        "        for _, row in dataframe.iterrows():\n",
        "            true_labels = parse_and_clean_codes(row[gold_standard])\n",
        "            predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "            tp = len(true_labels.intersection(predicted_labels))\n",
        "            fp = len(predicted_labels.difference(true_labels))\n",
        "            fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "            precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1_row = (2 * precision_row * recall_row / (precision_row + recall_row)\n",
        "                      if (precision_row + recall_row) > 0 else 0.0)\n",
        "            list_precision.append(precision_row)\n",
        "            list_recall.append(recall_row)\n",
        "            list_f1.append(f1_row)\n",
        "\n",
        "        results = {}\n",
        "        results['Micro-Precision'] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "        results['Micro-Recall']    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "        mp, mr = results['Micro-Precision'], results['Micro-Recall']\n",
        "        results['Micro-F1']        = 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0\n",
        "        results['Macro-Precision'] = float(np.mean(list_precision)) if list_precision else 0.0\n",
        "        results['Macro-Recall']    = float(np.mean(list_recall)) if list_recall else 0.0\n",
        "        results['Macro-F1']        = float(np.mean(list_f1)) if list_f1 else 0.0\n",
        "\n",
        "        final_results[model] = results\n",
        "\n",
        "    return pd.DataFrame(final_results).T\n",
        "\n",
        "# -----------------------------\n",
        "# ESTIMATIVAS PONTUAIS\n",
        "# -----------------------------\n",
        "print(\"Calculando as estimativas pontuais (6 métricas no dataset completo) [Leaf level]...\")\n",
        "point_estimates_df = calculate_all_metrics_micro_and_macro(df, model_cols, gold_standard_col)\n",
        "\n",
        "# -----------------------------\n",
        "# BOOTSTRAP\n",
        "# -----------------------------\n",
        "n_bootstraps = 10000\n",
        "print(f\"\\nIniciando o bootstrapping com {n_bootstraps} iterações para todas as 6 métricas [Leaf level]...\")\n",
        "bootstrap_scores = {model: {metric: [] for metric in metrics_to_calculate} for model in model_cols}\n",
        "\n",
        "for i in tqdm(range(n_bootstraps)):\n",
        "    df_resampled = df.sample(n=len(df), replace=True)\n",
        "    resampled_results = calculate_all_metrics_micro_and_macro(df_resampled, model_cols, gold_standard_col)\n",
        "    for model in model_cols:\n",
        "        for metric in metrics_to_calculate:\n",
        "            bootstrap_scores[model][metric].append(float(resampled_results.loc[model, metric]))\n",
        "\n",
        "print(\"Cálculo do bootstrapping concluído.\")\n",
        "\n",
        "# -----------------------------\n",
        "# CIs E TABELA FINAL\n",
        "# -----------------------------\n",
        "final_results_with_ci = {}\n",
        "alpha = (1.0 - 0.95) / 2.0\n",
        "\n",
        "for model in model_cols:\n",
        "    model_results = {}\n",
        "    for metric in metrics_to_calculate:\n",
        "        scores = bootstrap_scores[model][metric]\n",
        "        lower_bound = float(np.percentile(scores, alpha * 100))\n",
        "        upper_bound = float(np.percentile(scores, (1 - alpha) * 100))\n",
        "        model_results[metric] = float(point_estimates_df.loc[model, metric])\n",
        "        model_results[f'{metric} IC 95%'] = f\"[{lower_bound:.4f}, {upper_bound:.4f}]\"\n",
        "    final_results_with_ci[model] = model_results\n",
        "\n",
        "final_df = pd.DataFrame(final_results_with_ci).T\n",
        "final_df = final_df.sort_values(by='Micro-F1', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (MICRO E MACRO) COM INTERVALO DE CONFIANÇA (IC) — Leaf level\")\n",
        "print(\"=\"*80)\n",
        "print(final_df)\n",
        "\n",
        "# Salvar tabela com ICs (valores separados) no Drive\n",
        "final_csv_path = os.path.join(output_dir, \"final_results_with_CI.csv\")\n",
        "final_df.to_csv(final_csv_path, index=True)\n",
        "print(f\"\\nTabela final com IC salva: {final_csv_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# MAPEAMENTO DE NOMES PARA PLOTS\n",
        "# -----------------------------\n",
        "name_map = {\n",
        "    'API Maritalk - 25. High Evolution': 'Sabiá-3.1',\n",
        "    'API GPT-4o - 25. High Evolution': 'GPT-4o',\n",
        "    'API Deep-Seek - 25. High Evolution': 'DeepSeek-V3',\n",
        "}\n",
        "metric_label_map = {\n",
        "    'Micro-Precision': 'Micro Precision',\n",
        "    'Micro-Recall': 'Micro Recall',\n",
        "    'Micro-F1': 'Micro F1-Score',\n",
        "    'Macro-Precision': 'Macro Precision',\n",
        "    'Macro-Recall': 'Macro Recall',\n",
        "    'Macro-F1': 'Macro F1-Score',\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# PLOTS + SALVAMENTO\n",
        "# -----------------------------\n",
        "print(\"\\nGenerating distribution plots for each metric (Leaf level) and saving to Drive...\")\n",
        "for metric in metrics_to_calculate:\n",
        "    fig, axes = plt.subplots(1, len(model_cols), figsize=(len(model_cols) * 6, 5), sharey=True)\n",
        "\n",
        "    suptitle_txt = metric_label_map.get(metric, metric)\n",
        "    fig.suptitle(f'Bootstrap Distribution for {suptitle_txt}', fontsize=16)\n",
        "\n",
        "    # índice ordenado por Micro-F1 no final_df\n",
        "    for i, model in enumerate(final_df.index):\n",
        "        ax = axes[i]\n",
        "        scores = bootstrap_scores[model][metric]\n",
        "        sns.histplot(scores, kde=True, ax=ax, bins=30, stat=\"density\")\n",
        "\n",
        "        point_estimate = float(point_estimates_df.loc[model, metric])\n",
        "        ax.axvline(point_estimate, color='black', linestyle='-', linewidth=2,\n",
        "                   label=f'Point Estimate ({point_estimate:.4f})')\n",
        "\n",
        "        mean_bootstrap = float(np.mean(scores))\n",
        "        ax.axvline(mean_bootstrap, color='red', linestyle='--', linewidth=2,\n",
        "                   label=f'Bootstrap Mean ({mean_bootstrap:.4f})')\n",
        "\n",
        "        lower_bound = float(np.percentile(scores, 2.5))\n",
        "        upper_bound = float(np.percentile(scores, 97.5))\n",
        "        ax.axvline(lower_bound, color='g', linestyle=':', linewidth=2, label='95% CI')\n",
        "        ax.axvline(upper_bound, color='g', linestyle=':', linewidth=2)\n",
        "\n",
        "        ax.set_title(name_map.get(model, model), fontsize=12)\n",
        "        ax.set_xlabel(metric_label_map.get(metric, metric))\n",
        "        if i == 0:\n",
        "            ax.set_ylabel('Density')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    safe_metric = metric.replace(' ', '_').replace('-', '')\n",
        "    fig_path = os.path.join(output_dir, f'distribution_bootstrap_{safe_metric}.png')\n",
        "    plt.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"Chart saved: {fig_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# PE vs BOOTSTRAP MEAN + SALVAR\n",
        "# -----------------------------\n",
        "rows = []\n",
        "for model in model_cols:\n",
        "    # garantir chave correta se houver variação em 'API GPT-4o'\n",
        "    model_key = model\n",
        "    if model_key not in point_estimates_df.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df.index:\n",
        "        model_key = 'API GPT-4o'\n",
        "    if model_key not in point_estimates_df.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df.index:\n",
        "        model_key = 'API GPT-4o'\n",
        "\n",
        "    for metric in metrics_to_calculate:\n",
        "        pe = float(point_estimates_df.loc[model_key, metric])\n",
        "        bm = float(np.mean(bootstrap_scores[model_key][metric]))\n",
        "        diff = bm - pe\n",
        "        rows.append({\n",
        "            'Model': name_map.get(model_key, model_key),\n",
        "            'Metric': metric_label_map.get(metric, metric),\n",
        "            'Point Estimate': f\"{pe:.4f}\",\n",
        "            'Bootstrap Mean': f\"{bm:.4f}\",\n",
        "            'Diff (BM-PE)': f\"{diff:.6f}\",\n",
        "        })\n",
        "\n",
        "pe_vs_bm_df = pd.DataFrame(rows)\n",
        "pe_vs_bm_df['abs_diff'] = pe_vs_bm_df['Diff (BM-PE)'].astype(float).abs()\n",
        "pe_vs_bm_df = pe_vs_bm_df.sort_values(by='abs_diff', ascending=False).drop(columns='abs_diff')\n",
        "\n",
        "pebm_csv_path = os.path.join(output_dir, \"point_estimate_vs_bootstrap_mean.csv\")\n",
        "pe_vs_bm_df.to_csv(pebm_csv_path, index=False)\n",
        "print(f\"\\nComparação PE vs BM salva: {pebm_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0fN4XReJkuq"
      },
      "source": [
        "**Three-Character Category - Data Cleasing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GYcDBsW6WHF1"
      },
      "outputs": [],
      "source": [
        "# --- Etapa 3: Funções de Limpeza e Transformação ---\n",
        "\n",
        "df2 = df.copy()\n",
        "\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    \"\"\"Lê uma célula (texto ou lista) e retorna um conjunto de códigos limpos.\"\"\"\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = cell_content.strip().replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '')\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "def truncate_codes_in_cell(cell_content):\n",
        "    \"\"\"Usa a função anterior para ler os códigos e retorna uma NOVA LISTA com os códigos truncados.\"\"\"\n",
        "    full_codes_set = parse_and_clean_codes(cell_content)\n",
        "    # Pega cada código do conjunto, fatia os 3 primeiros caracteres e retorna como uma lista\n",
        "    truncated_codes_list = [code[:3] for code in full_codes_set]\n",
        "    return truncated_codes_list\n",
        "\n",
        "\n",
        "# --- Etapa 4: Aplicar a Transformação em df2 ---\n",
        "\n",
        "# Lista de todas as colunas que contêm códigos CID\n",
        "columns_to_transform = [\n",
        "    '21. CID de Alta', 'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution']\n",
        "\n",
        "print(\"Iniciando a transformação para remover a especificidade (3 caracteres)...\")\n",
        "\n",
        "for col in columns_to_transform:\n",
        "    if col in df2.columns:\n",
        "        # Aplica a função de truncar em cada célula da coluna especificada\n",
        "        df2[col] = df2[col].apply(truncate_codes_in_cell)\n",
        "        print(f\"Coluna '{col}' transformada.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  DATA FRAME ORIGINAL (df)\")\n",
        "print(\"=\"*50)\n",
        "print(df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  NOVO DATAFRAME (df2) - SEM ESPECIFICIDADE (3 CARACTERES)\")\n",
        "print(\"=\"*50)\n",
        "print(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTBfCf2qKJHM"
      },
      "source": [
        "**Three Category Metrics Bootstrap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoq8AJJlKVrT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURAÇÕES INICIAIS\n",
        "# -----------------------------\n",
        "# >>>> Garanta que 'df2' já esteja carregado com as colunas abaixo <<<<\n",
        "gold_standard_col = '21. CID de Alta'\n",
        "model_cols = [\n",
        "    'API Maritalk - 25. High Evolution',\t'API GPT-4o - 25. High Evolution', 'API Deep-Seek - 25. High Evolution'\n",
        "]\n",
        "metrics_to_calculate = [\n",
        "    'Micro-Precision', 'Micro-Recall', 'Micro-F1',\n",
        "    'Macro-Precision', 'Macro-Recall', 'Macro-F1'\n",
        "]\n",
        "\n",
        "# Diretório de saída (Tree Category level)\n",
        "output_dir = \"/content/drive/MyDrive/Doutorado - Ricardo- Após a Qualificação/Paper: EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATED ICD-10 CODING OF OBSTETRIC CLINICAL NOTES IN PORTUGUESE: A COMPARATIVE STUDY/Bootstrap/Three-character category\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# FUNÇÕES AUXILIARES\n",
        "# -----------------------------\n",
        "def parse_and_clean_codes(cell_content):\n",
        "    if isinstance(cell_content, (list, np.ndarray)):\n",
        "        return {str(code).strip().upper() for code in cell_content if not pd.isna(code)}\n",
        "    if pd.isna(cell_content) or not isinstance(cell_content, str) or not cell_content.strip():\n",
        "        return set()\n",
        "    cleaned_text = (cell_content.strip()\n",
        "                               .replace('[', '')\n",
        "                               .replace(']', '')\n",
        "                               .replace(\"'\", \"\")\n",
        "                               .replace('\"', ''))\n",
        "    if not cleaned_text:\n",
        "        return set()\n",
        "    codes = {code.strip().upper() for code in cleaned_text.split(',')}\n",
        "    return codes\n",
        "\n",
        "def calculate_all_metrics_micro_and_macro(dataframe, models, gold_standard):\n",
        "    final_results = {}\n",
        "    for model in models:\n",
        "        total_tp, total_fp, total_fn = 0, 0, 0\n",
        "        list_precision, list_recall, list_f1 = [], [], []\n",
        "\n",
        "        for _, row in dataframe.iterrows():\n",
        "            true_labels = parse_and_clean_codes(row[gold_standard])\n",
        "            predicted_labels = parse_and_clean_codes(row.get(model))\n",
        "\n",
        "            tp = len(true_labels.intersection(predicted_labels))\n",
        "            fp = len(predicted_labels.difference(true_labels))\n",
        "            fn = len(true_labels.difference(predicted_labels))\n",
        "\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "            precision_row = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            recall_row = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1_row = (2 * precision_row * recall_row / (precision_row + recall_row)\n",
        "                      if (precision_row + recall_row) > 0 else 0.0)\n",
        "            list_precision.append(precision_row)\n",
        "            list_recall.append(recall_row)\n",
        "            list_f1.append(f1_row)\n",
        "\n",
        "        results = {}\n",
        "        results['Micro-Precision'] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "        results['Micro-Recall']    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "        mp, mr = results['Micro-Precision'], results['Micro-Recall']\n",
        "        results['Micro-F1']        = 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0\n",
        "        results['Macro-Precision'] = float(np.mean(list_precision)) if list_precision else 0.0\n",
        "        results['Macro-Recall']    = float(np.mean(list_recall)) if list_recall else 0.0\n",
        "        results['Macro-F1']        = float(np.mean(list_f1)) if list_f1 else 0.0\n",
        "\n",
        "        final_results[model] = results\n",
        "\n",
        "    return pd.DataFrame(final_results).T\n",
        "\n",
        "# -----------------------------\n",
        "# ESTIMATIVAS PONTUAIS\n",
        "# -----------------------------\n",
        "print(\"Calculando as estimativas pontuais (6 métricas no dataset completo) [Leaf level]...\")\n",
        "point_estimates_df2 = calculate_all_metrics_micro_and_macro(df2, model_cols, gold_standard_col)\n",
        "\n",
        "# -----------------------------\n",
        "# BOOTSTRAP\n",
        "# -----------------------------\n",
        "n_bootstraps = 10000\n",
        "print(f\"\\nIniciando o bootstrapping com {n_bootstraps} iterações para todas as 6 métricas [Leaf level]...\")\n",
        "bootstrap_scores = {model: {metric: [] for metric in metrics_to_calculate} for model in model_cols}\n",
        "\n",
        "for i in tqdm(range(n_bootstraps)):\n",
        "    df2_resampled = df2.sample(n=len(df2), replace=True)\n",
        "    resampled_results = calculate_all_metrics_micro_and_macro(df2_resampled, model_cols, gold_standard_col)\n",
        "    for model in model_cols:\n",
        "        for metric in metrics_to_calculate:\n",
        "            bootstrap_scores[model][metric].append(float(resampled_results.loc[model, metric]))\n",
        "\n",
        "print(\"Cálculo do bootstrapping concluído.\")\n",
        "\n",
        "# -----------------------------\n",
        "# CIs E TABELA FINAL\n",
        "# -----------------------------\n",
        "final_results_with_ci = {}\n",
        "alpha = (1.0 - 0.95) / 2.0\n",
        "\n",
        "for model in model_cols:\n",
        "    model_results = {}\n",
        "    for metric in metrics_to_calculate:\n",
        "        scores = bootstrap_scores[model][metric]\n",
        "        lower_bound = float(np.percentile(scores, alpha * 100))\n",
        "        upper_bound = float(np.percentile(scores, (1 - alpha) * 100))\n",
        "        model_results[metric] = float(point_estimates_df2.loc[model, metric])\n",
        "        model_results[f'{metric} IC 95%'] = f\"[{lower_bound:.4f}, {upper_bound:.4f}]\"\n",
        "    final_results_with_ci[model] = model_results\n",
        "\n",
        "final_df2 = pd.DataFrame(final_results_with_ci).T\n",
        "final_df2 = final_df2.sort_values(by='Micro-F1', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  DESEMPENHO FINAL DOS MODELOS (MICRO E MACRO) COM INTERVALO DE CONFIANÇA (IC) — Leaf level\")\n",
        "print(\"=\"*80)\n",
        "print(final_df2)\n",
        "\n",
        "# Salvar tabela com ICs (valores separados) no Drive\n",
        "final_csv_path = os.path.join(output_dir, \"final_results_with_CI.csv\")\n",
        "final_df2.to_csv(final_csv_path, index=True)\n",
        "print(f\"\\nTabela final com IC salva: {final_csv_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# MAPEAMENTO DE NOMES PARA PLOTS\n",
        "# -----------------------------\n",
        "name_map = {\n",
        "    'API Maritalk - 25. High Evolution': 'Sabiá-3.1',\n",
        "    'API GPT-4o - 25. High Evolution': 'GPT-4o',\n",
        "    'API Deep-Seek - 25. High Evolution': 'DeepSeek-V3',\n",
        "}\n",
        "\n",
        "metric_label_map = {\n",
        "    'Micro-Precision': 'Micro Precision',\n",
        "    'Micro-Recall': 'Micro Recall',\n",
        "    'Micro-F1': 'Micro F1-Score',\n",
        "    'Macro-Precision': 'Macro Precision',\n",
        "    'Macro-Recall': 'Macro Recall',\n",
        "    'Macro-F1': 'Macro F1-Score',\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# PLOTS + SALVAMENTO\n",
        "# -----------------------------\n",
        "print(\"\\nGenerating distribution plots for each metric (Leaf level) and saving to Drive...\")\n",
        "for metric in metrics_to_calculate:\n",
        "    fig, axes = plt.subplots(1, len(model_cols), figsize=(len(model_cols) * 6, 5), sharey=True)\n",
        "\n",
        "    suptitle_txt = metric_label_map.get(metric, metric)\n",
        "    fig.suptitle(f'Bootstrap Distribution for {suptitle_txt}', fontsize=16)\n",
        "\n",
        "    # índice ordenado por Micro-F1 no final_df2\n",
        "    for i, model in enumerate(final_df2.index):\n",
        "        ax = axes[i]\n",
        "        scores = bootstrap_scores[model][metric]\n",
        "        sns.histplot(scores, kde=True, ax=ax, bins=30, stat=\"density\")\n",
        "\n",
        "        point_estimate = float(point_estimates_df2.loc[model, metric])\n",
        "        ax.axvline(point_estimate, color='black', linestyle='-', linewidth=2,\n",
        "                   label=f'Point Estimate ({point_estimate:.4f})')\n",
        "\n",
        "        mean_bootstrap = float(np.mean(scores))\n",
        "        ax.axvline(mean_bootstrap, color='red', linestyle='--', linewidth=2,\n",
        "                   label=f'Bootstrap Mean ({mean_bootstrap:.4f})')\n",
        "\n",
        "        lower_bound = float(np.percentile(scores, 2.5))\n",
        "        upper_bound = float(np.percentile(scores, 97.5))\n",
        "        ax.axvline(lower_bound, color='g', linestyle=':', linewidth=2, label='95% CI')\n",
        "        ax.axvline(upper_bound, color='g', linestyle=':', linewidth=2)\n",
        "\n",
        "        ax.set_title(name_map.get(model, model), fontsize=12)\n",
        "        ax.set_xlabel(metric_label_map.get(metric, metric))\n",
        "        if i == 0:\n",
        "            ax.set_ylabel('Density')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    safe_metric = metric.replace(' ', '_').replace('-', '')\n",
        "    fig_path = os.path.join(output_dir, f'distribution_bootstrap_{safe_metric}.png')\n",
        "    plt.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"Chart saved: {fig_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# PE vs BOOTSTRAP MEAN + SALVAR\n",
        "# -----------------------------\n",
        "rows = []\n",
        "for model in model_cols:\n",
        "    # garantir chave correta se houver variação em 'API GPT-4o'\n",
        "    model_key = model\n",
        "    if model_key not in point_estimates_df2.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df2.index:\n",
        "        model_key = 'API GPT-4o'\n",
        "    if model_key not in point_estimates_df2.index and model_key == 'API GPT-4o' and 'API GPT-4o' in point_estimates_df2.index:\n",
        "        model_key = 'API GPT-4o'\n",
        "\n",
        "    for metric in metrics_to_calculate:\n",
        "        pe = float(point_estimates_df2.loc[model_key, metric])\n",
        "        bm = float(np.mean(bootstrap_scores[model_key][metric]))\n",
        "        diff = bm - pe\n",
        "        rows.append({\n",
        "            'Model': name_map.get(model_key, model_key),\n",
        "            'Metric': metric_label_map.get(metric, metric),\n",
        "            'Point Estimate': f\"{pe:.4f}\",\n",
        "            'Bootstrap Mean': f\"{bm:.4f}\",\n",
        "            'Diff (BM-PE)': f\"{diff:.6f}\",\n",
        "        })\n",
        "\n",
        "pe_vs_bm_df2 = pd.DataFrame(rows)\n",
        "pe_vs_bm_df2['abs_diff'] = pe_vs_bm_df2['Diff (BM-PE)'].astype(float).abs()\n",
        "pe_vs_bm_df2 = pe_vs_bm_df2.sort_values(by='abs_diff', ascending=False).drop(columns='abs_diff')\n",
        "\n",
        "pebm_csv_path = os.path.join(output_dir, \"point_estimate_vs_bootstrap_mean.csv\")\n",
        "pe_vs_bm_df2.to_csv(pebm_csv_path, index=False)\n",
        "print(f\"\\nComparação PE vs BM salva: {pebm_csv_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
